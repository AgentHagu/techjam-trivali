{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f93057c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "\n",
    "# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "# %pip install -qU \"langchain[openai]\"\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install -qU langchain-core\n",
    "# %pip install --upgrade --quiet langgraph langchain-community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a725f239-86b5-4d9c-b592-987520eb2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "74220954-0d45-4bca-a5dd-64b02e95ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_CHAT_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_CHAT_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "574f6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "735e8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "16bb5e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# LOAD CONTEXT\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load and chunk contents of the bills and policies\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB976\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"tab_content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "95ba7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD TOOLS FOR GRAPH\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "79fd7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD AND COMPILE GRAPH\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "13a8daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Does the following feature comply with the regulations stated? Feature: Universal PF deactivation on guest mode. Description: By default, PF will be turned off for all uses browsing in guest mode.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_vvquaOtcRLc6o5eaeaKwoEGc)\n",
      " Call ID: call_vvquaOtcRLc6o5eaeaKwoEGc\n",
      "  Args:\n",
      "    query: universal personalized feed deactivation guest mode regulations\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB976', 'title': 'SB976:v91#DOCUMENT', 'language': 'No language found.'}\n",
      "Content: by the parent. This setting shall be set by the operator as on by default, in a manner in which the child’s access is limited between the hours of 12 a.m. and 6 a.m., in the user’s local time zone.(2) Limit their child’s access to any addictive feed from the addictive internet-based service or application to a length of time per day specified by the verified parent. This setting shall be set by the operator as on by default, in a manner in which the child’s access is limited to one hour per day unless modified by the verified parent.(3) Limit their child’s ability to view the number of likes or other forms of feedback to pieces of media within an addictive feed. This setting shall be set by the operator as on by default.(4) Require\n",
      "\t\t\t\t\t\t  that the default feed provided to the child when\n",
      "\n",
      "Source: {'source': 'https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB976', 'title': 'SB976:v91#DOCUMENT', 'language': 'No language found.'}\n",
      "Content: entering the internet-based service or application be one in which pieces of media are not recommended, selected, or prioritized for display based on information provided by the user, or otherwise associated with the user or the user’s device, other than the user’s age or status as a minor.(5) Set their child’s account to private mode, in a manner in which only users to whom the child is connected on the addictive internet-based service or application may view or respond to content posted by the child. This setting shall be set by the operator as on by default.27003. (a) This chapter shall not be construed as requiring the operator of an addictive internet-based service or application to give a parent any additional or special access to, or control over, the data or accounts of their child.(b) This chapter shall not be construed as preventing any action taken in good faith to restrict access to, or availability of, media.27004. (a) An operator may choose not to provide\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, this feature complies with the regulations stated. By default turning off the Personalized Feed (PF) in guest mode prevents the collection of personal data and tailored content for users who are not logged in, aligning with privacy protections. This ensures that users' browsing experiences in guest mode remain unmonitored and free from personalized content based on their activity.\n"
     ]
    }
   ],
   "source": [
    "# TEST INPUT\n",
    "\n",
    "input_message = \"Does the following feature comply with the regulations stated? Feature: Universal PF deactivation on guest mode. Description: By default, PF will be turned off for all uses browsing in guest mode.\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
