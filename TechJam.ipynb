{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93057c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "\n",
    "# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "# %pip install -qU \"langchain[openai]\"\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install -qU langchain-core\n",
    "# %pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\n",
    "# %pip install gradio\n",
    "# %pip install langchain-chroma\n",
    "# %pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725f239-86b5-4d9c-b592-987520eb2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74220954-0d45-4bca-a5dd-64b02e95ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_CHAT_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_CHAT_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "policy_vector_store = Chroma(persist_directory=\"./policy_vector_store\", embedding_function=embeddings)\n",
    "terminology_vector_store = Chroma(persist_directory=\"./terminology_vector_store\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONTEXT\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_policy(query: str):\n",
    "    \"\"\"Use this tool when you need to check compliance of a feature\n",
    "    against official regulations and policies.\"\"\"\n",
    "    retrieved_docs = policy_vector_store.similarity_search(query, k=5)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_terminology(query: str):\n",
    "    \"\"\"Use this tool when the user query or context contains acronyms or terms\n",
    "    that need clarification before checking compliance.\"\"\"\n",
    "    retrieved_docs = terminology_vector_store.similarity_search(query, k=10)\n",
    "    serialized = \"\\n\".join(\n",
    "        f\"{doc.metadata['term']}: {doc.page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve_policy])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve_policy])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "    \"\"\"\n",
    "        You are an assistant for feature compliance checking. \n",
    "        Use the following pieces of retrieved context to flag\n",
    "        whether this feature needs geo-specific compliance logic\n",
    "        with clear reasoning. Optionally, provide the related \n",
    "        regulations. If you don't know the answer, say that you\n",
    "        don't know. Use three sentences maximum and keep the\n",
    "        answer concise.\n",
    "        \\n\\n\n",
    "    \"\"\"\n",
    "    f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD AND COMPILE GRAPH\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "\n",
    "# graph_builder.add_node(query_or_respond)\n",
    "# graph_builder.add_node(tools)\n",
    "# graph_builder.add_node(generate)\n",
    "\n",
    "# graph_builder.set_entry_point(\"query_or_respond\")\n",
    "# graph_builder.add_conditional_edges(\n",
    "#     \"query_or_respond\",\n",
    "#     tools_condition,\n",
    "#     {END: END, \"tools\": \"tools\"},\n",
    "# )\n",
    "# graph_builder.add_edge(\"tools\", \"generate\")\n",
    "# graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(llm, [retrieve_policy, retrieve_terminology], checkpointer=memory)\n",
    "# graph = graph_builder.compile(checkpointer=memory)\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST INPUT\n",
    "\n",
    "# input_message = \"Does the following feature comply with the regulations stated? Feature: Universal PF deactivation on guest mode. Description: By default, PF will be turned off for all uses browsing in guest mode.\"\n",
    "\n",
    "# for step in graph.stream(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "#     stream_mode=\"values\",\n",
    "#     config=config,\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def rag_chat(user_message, history):\n",
    "    responses = []\n",
    "    old_step_messages = []\n",
    "    for step in agent_executor.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_message}]},\n",
    "        stream_mode=\"values\",\n",
    "        config=config,\n",
    "    ):\n",
    "        if \"messages\" in step:\n",
    "            responses.append(step[\"messages\"][-1].content)\n",
    "\n",
    "            # For debugging\n",
    "            message_count_diff = len(step[\"messages\"]) - len(old_step_messages)\n",
    "            if message_count_diff > 0:\n",
    "                new_messages = step[\"messages\"][-message_count_diff:]\n",
    "                for msg in new_messages:\n",
    "                    msg.pretty_print()\n",
    "                old_step_messages = step[\"messages\"]\n",
    "\n",
    "    reply = responses[-1] if responses else \"No response.\"\n",
    "    return reply\n",
    "\n",
    "demo = gr.ChatInterface(rag_chat, title=\"Trivali Feature Compliance Checker\")\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
